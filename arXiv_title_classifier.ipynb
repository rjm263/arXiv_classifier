{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:46:53.188340Z",
     "start_time": "2025-09-02T15:46:45.534886Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW, SGD\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from safetensors.torch import save_file, load_file"
   ],
   "id": "fdb72d254fbd29e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing arXiv data\n",
    "\n",
    "Here we load the arXiv data set arxiv-metadata-oai-snapshot.json from Kaggle (https://www.kaggle.com/datasets/Cornell-University/arxiv/data) into a pandas DataFrame and perform the following cleaning operations:\n",
    "\n",
    "- restrict to title, abstract, categories columns\n",
    "- restrict to the num_labels=20 most popular categories\n",
    "- perform balanced sampling of papers such that each category appears exactly N=1000 times\n",
    "\n",
    "NB: the code can easily be adapted to include more categories for classification."
   ],
   "id": "188cbec02a9ba203"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:47:53.312830Z",
     "start_time": "2025-09-02T15:46:53.193158Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "# Load the data in chunks and filter right away (easier on memory and time)\n",
    "path = \"../data_sets/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "columns = [\"title\", \"abstract\", \"categories\"]\n",
    "chunks_filt = []\n",
    "\n",
    "for chunk in pd.read_json(path, lines=True, chunksize=10000):\n",
    "    chunk_filt = chunk[columns]\n",
    "    chunks_filt.append(chunk_filt)\n",
    "\n",
    "data = pd.concat(chunks_filt, ignore_index=True)"
   ],
   "id": "c73e786c08d85284"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:47:56.922738Z",
     "start_time": "2025-09-02T15:47:53.439538Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract          categories  \n",
       "0    A fully differential calculation in perturba...            [hep-ph]  \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    [math.CO, cs.CG]  \n",
       "2    The evolution of Earth-Moon system is descri...    [physics.gen-ph]  \n",
       "3    We show that a determinant of Stirling cycle...           [math.CO]  \n",
       "4    In this paper we show how to compute the $\\L...  [math.CA, math.FA]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[hep-ph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[math.CO, cs.CG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[physics.gen-ph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[math.CO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[math.CA, math.FA]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3,
   "source": [
    "# Split the categories string into a list of single categories\n",
    "data.categories = data.categories.str.split(\" \")\n",
    "data.head()"
   ],
   "id": "5a73579c35931186"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:00:02.637545Z",
     "start_time": "2025-09-03T11:59:59.501981Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories\n",
       "cs.LG                 223626\n",
       "hep-ph                188414\n",
       "hep-th                174604\n",
       "quant-ph              163662\n",
       "cs.CV                 159690\n",
       "cs.AI                 132333\n",
       "gr-qc                 114478\n",
       "astro-ph              105380\n",
       "cond-mat.mtrl-sci     100648\n",
       "cond-mat.mes-hall      96044\n",
       "cs.CL                  87718\n",
       "math.MP                84637\n",
       "math-ph                84636\n",
       "cond-mat.str-el        78278\n",
       "cond-mat.stat-mech     77357\n",
       "astro-ph.CO            72188\n",
       "math.CO                71920\n",
       "stat.ML                71882\n",
       "astro-ph.GA            70770\n",
       "math.AP                67927\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17,
   "source": [
    "# Check for the most frequently published under categories (including cross-listing)\n",
    "num_labels = 20\n",
    "cat_ranking_cross = data.categories.explode().value_counts()\n",
    "\n",
    "cats = cat_ranking_cross.index.tolist()[:num_labels]\n",
    "cat_ranking_cross.iloc[:num_labels]"
   ],
   "id": "fff7704bbe24aeb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T13:46:10.163633Z",
     "start_time": "2025-09-03T13:46:08.541924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Just for fun: check how many papers are listed under how many different categories\n",
    "data.categories.map(lambda s: len(s)).value_counts()"
   ],
   "id": "27dd81f6549508c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories\n",
       "1     1458652\n",
       "2      829424\n",
       "3      335912\n",
       "4      108365\n",
       "5       30931\n",
       "6        6672\n",
       "7         961\n",
       "8         161\n",
       "9          33\n",
       "10         14\n",
       "11          2\n",
       "13          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T13:46:26.841111Z",
     "start_time": "2025-09-03T13:46:25.698590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A paper with 13 cross-listed categories?! I have to know what it's about...\n",
    "idx = data.categories.map(lambda s: len(s)).idxmax()\n",
    "data.loc[idx,:]"
   ],
   "id": "212c6b9fbab143e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title         The finite harmonic oscillator and its associa...\n",
       "abstract        A system of functions (signals) on the finit...\n",
       "categories    [cs.IT, cs.CR, cs.DM, math-ph, math.GR, math.I...\n",
       "Name: 77549, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:09.761250Z",
     "start_time": "2025-09-02T15:47:58.043787Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "# Perform balanced sampling on the dataset, so as to retain each top 20 category exactly N times (to avoid bias in training)\n",
    "N = 1000\n",
    "\n",
    "cat_count = pd.Series([0] * len(cats), index=cats)\n",
    "selected_rows = []\n",
    "shuffled_data = data.sample(frac=1, random_state=42)\n",
    "\n",
    "for i, row in shuffled_data.iterrows():\n",
    "    top_cats = set(row.categories) & set(cats)\n",
    "\n",
    "    if len(top_cats) < len(row.categories): continue\n",
    "\n",
    "    if all(cat_count[cat] < N for cat in top_cats):\n",
    "        selected_rows.append(row)\n",
    "        for cat in top_cats:\n",
    "            cat_count[cat] += 1\n",
    "\n",
    "    if all(cat_count[cat] >= N for cat in cats):\n",
    "        break\n",
    "\n",
    "arXiv_data_cross = pd.DataFrame(selected_rows).sample(frac=1, random_state=42).reset_index(drop=True)"
   ],
   "id": "c0426589d95ca3e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:10.716113Z",
     "start_time": "2025-09-02T15:49:09.919118Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "# Split the data set into training and test sets; here the train share is 80%\n",
    "n_train = .8\n",
    "\n",
    "train_data_split = arXiv_data_cross.explode('categories').groupby('categories', group_keys=False).head(n_train * N)\n",
    "train_data = train_data_split.groupby(train_data_split.index).agg(lambda x: list(x) if x.name == 'categories' else x.iloc[0])\n",
    "\n",
    "test_data_split = arXiv_data_cross.explode('categories').groupby('categories', group_keys=False).tail(N - n_train * N)\n",
    "test_data = test_data_split.groupby(test_data_split.index).agg(lambda x: list(x) if x.name == 'categories' else x.iloc[0])"
   ],
   "id": "1891d2a88cd4d1f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T13:49:27.509681Z",
     "start_time": "2025-09-03T13:49:22.910914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check that each top 20 category appears exactly 800 times\n",
    "train_data.explode('categories').categories.value_counts()"
   ],
   "id": "f901bc4d906d89fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories\n",
       "cond-mat.mtrl-sci     800\n",
       "stat.ML               800\n",
       "cs.CV                 800\n",
       "hep-th                800\n",
       "quant-ph              800\n",
       "gr-qc                 800\n",
       "astro-ph.CO           800\n",
       "hep-ph                800\n",
       "math.CO               800\n",
       "astro-ph              800\n",
       "astro-ph.GA           800\n",
       "cs.AI                 800\n",
       "cs.CL                 800\n",
       "cond-mat.str-el       800\n",
       "cond-mat.mes-hall     800\n",
       "math.MP               800\n",
       "math-ph               800\n",
       "cond-mat.stat-mech    800\n",
       "math.AP               800\n",
       "cs.LG                 800\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training and Testing\n",
    "\n",
    "Now we can start preparing the chosen data for training/testing of the chosen model---here it's distilbert-base-cased: (relatively) lightweight, cased in order to recognize terms named after people (e.g. Hall). For training/testing we use the Hugging Face trainer API\n",
    "\n",
    "- First, we turn our sample arXiv data into a Hugging Face Dataset, using both title and abstract; however, since we only want to test with titles, we drop abstracts with probability p=0.2\n",
    "- Since each paper can be listed under multiple categories, this is a multi-label classification model; we use multi-hot encoded labels (i.e. vectors of dim=N=20)\n",
    "- Instead of choosing a single category as the winner (using torch.argmax), we look at probabilities for each category (torch.sigmoid) and choose a cutoff at 50%"
   ],
   "id": "67f8ee13c460cbab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:10.730356Z",
     "start_time": "2025-09-02T15:49:10.725911Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "# Turn arXiv data into torch Dataset class\n",
    "class ArXivDatasetHF(Dataset):\n",
    "    def __init__(self, text, target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dct = {key: val[idx] for key, val in self.text.items()}\n",
    "        dct[\"labels\"] = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        return dct"
   ],
   "id": "40295099dde69004"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:10.745438Z",
     "start_time": "2025-09-02T15:49:10.740640Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "# Translate between encoded and decoded labels\n",
    "label2id = {key: val for val, key in enumerate(sorted(cats))}\n",
    "id2label = {val: key for key, val in label2id.items()}\n",
    "\n",
    "# Encode labels with multi-het\n",
    "def label_encoder(data_frame):\n",
    "    labels = []\n",
    "    for label in data_frame.categories.tolist():\n",
    "        id = [0] * len(cats)\n",
    "        for l in label:\n",
    "            id[label2id[l]] = 1\n",
    "        labels.append(id)\n",
    "    return np.array(labels)\n",
    "\n",
    "# Tokenize text input\n",
    "def feature_encoder(data_frame, tokenize, max_len, p):\n",
    "    # Drop abstract w/ prob p\n",
    "    li = []\n",
    "    for i, row in data_frame.iterrows():\n",
    "        rd = random.random()\n",
    "        if rd > p:\n",
    "            # NB: we include Title: and Abstract: in front, hoping the model would learn to understand what we provide\n",
    "            li.append(f\"Title: {row.title}, Abstract: {row.abstract}\")\n",
    "        else:\n",
    "            li.append(f\"Title: {row.title}, Abstract: \")\n",
    "\n",
    "    return tokenize(\n",
    "            li,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len\n",
    "        )"
   ],
   "id": "df2985ecedcd0cd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:34.496594Z",
     "start_time": "2025-09-02T15:49:10.754090Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "# Choose model data\n",
    "model_name = 'distilbert-base-cased'  # 'tbs17/MathBERT'\n",
    "max_length = 512\n",
    "batch_size = 8\n",
    "torch.manual_seed(42)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Encode labels, features from our train/test datasets prepared above and convert to HF Dataset\n",
    "labels = label_encoder(train_data)\n",
    "features = feature_encoder(train_data, tokenizer, max_length, p=0.2)\n",
    "train_sample = ArXivDatasetHF(features, labels)\n",
    "train_loader = DataLoader(train_sample, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "features = feature_encoder(test_data, tokenizer, max_length, p=1.0)   # p=1 --> no abstracts in the test sample\n",
    "labels = label_encoder(test_data)\n",
    "test_sample = ArXivDatasetHF(features, labels)\n",
    "test_loader = DataLoader(test_sample, batch_size=batch_size, shuffle=False)"
   ],
   "id": "db25ea99488d574f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:34.908786Z",
     "start_time": "2025-09-02T15:49:34.571915Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "# Set up model to use; we choose 'multi_label_classification' for the problem type\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type='multi_label_classification',\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "# Define metrics used to evaluate model performance; accuracy is subset accuracy for each label (appropriate for multi-label); F1 score measures precision and recall for multi-label predictions\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(labels.reshape(-1), preds.reshape(-1))\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1_score\": f1}"
   ],
   "id": "646c5a79e4407873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:49:45.735671Z",
     "start_time": "2025-09-02T15:49:45.022875Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "# Define arguments for training/testing and HF Trainer\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=1e-2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    report_to=[\"none\"],\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_sample,\n",
    "    eval_dataset=test_sample,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "96d08497aff2a953"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T09:01:11.693166Z",
     "start_time": "2025-09-02T15:49:46.260115Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7625' max='7625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7625/7625 17:11:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>0.407774</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.303306</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.266671</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.249959</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.242081</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.235767</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.229420</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>0.223043</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.217483</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>0.211378</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.207644</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.201036</td>\n",
       "      <td>0.935171</td>\n",
       "      <td>0.006479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.194288</td>\n",
       "      <td>0.935415</td>\n",
       "      <td>0.013903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.191345</td>\n",
       "      <td>0.935463</td>\n",
       "      <td>0.015869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.186648</td>\n",
       "      <td>0.935902</td>\n",
       "      <td>0.030497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.188626</td>\n",
       "      <td>0.935610</td>\n",
       "      <td>0.023669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>0.227342</td>\n",
       "      <td>0.935285</td>\n",
       "      <td>0.014364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.185536</td>\n",
       "      <td>0.936325</td>\n",
       "      <td>0.050436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.160300</td>\n",
       "      <td>0.189243</td>\n",
       "      <td>0.936602</td>\n",
       "      <td>0.052491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.183319</td>\n",
       "      <td>0.935691</td>\n",
       "      <td>0.025142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.180582</td>\n",
       "      <td>0.936602</td>\n",
       "      <td>0.062515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.188398</td>\n",
       "      <td>0.936309</td>\n",
       "      <td>0.049041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.173445</td>\n",
       "      <td>0.937333</td>\n",
       "      <td>0.084561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.173768</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.083294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.172239</td>\n",
       "      <td>0.937626</td>\n",
       "      <td>0.093144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.171262</td>\n",
       "      <td>0.936699</td>\n",
       "      <td>0.067098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.168192</td>\n",
       "      <td>0.937626</td>\n",
       "      <td>0.096136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.163274</td>\n",
       "      <td>0.938732</td>\n",
       "      <td>0.147125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.165857</td>\n",
       "      <td>0.939187</td>\n",
       "      <td>0.147287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.167623</td>\n",
       "      <td>0.938699</td>\n",
       "      <td>0.138089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.277384</td>\n",
       "      <td>0.935220</td>\n",
       "      <td>0.025917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.167571</td>\n",
       "      <td>0.940195</td>\n",
       "      <td>0.224051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.164042</td>\n",
       "      <td>0.939805</td>\n",
       "      <td>0.182059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.163767</td>\n",
       "      <td>0.940065</td>\n",
       "      <td>0.185954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.165210</td>\n",
       "      <td>0.940276</td>\n",
       "      <td>0.207551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.168411</td>\n",
       "      <td>0.941203</td>\n",
       "      <td>0.236164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.942049</td>\n",
       "      <td>0.277079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.165916</td>\n",
       "      <td>0.941317</td>\n",
       "      <td>0.257560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.155066</td>\n",
       "      <td>0.942179</td>\n",
       "      <td>0.256378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.151779</td>\n",
       "      <td>0.942911</td>\n",
       "      <td>0.310622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.155353</td>\n",
       "      <td>0.941870</td>\n",
       "      <td>0.270259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>0.151172</td>\n",
       "      <td>0.943106</td>\n",
       "      <td>0.302571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.151009</td>\n",
       "      <td>0.943496</td>\n",
       "      <td>0.323008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.151658</td>\n",
       "      <td>0.943545</td>\n",
       "      <td>0.323461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.149386</td>\n",
       "      <td>0.943593</td>\n",
       "      <td>0.341621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.151592</td>\n",
       "      <td>0.943512</td>\n",
       "      <td>0.314793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.148771</td>\n",
       "      <td>0.943837</td>\n",
       "      <td>0.378330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.147332</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.362754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.146240</td>\n",
       "      <td>0.944569</td>\n",
       "      <td>0.374381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>0.148465</td>\n",
       "      <td>0.942390</td>\n",
       "      <td>0.344738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.943545</td>\n",
       "      <td>0.339170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.942911</td>\n",
       "      <td>0.373148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.146743</td>\n",
       "      <td>0.944455</td>\n",
       "      <td>0.393896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.146886</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.387414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.144269</td>\n",
       "      <td>0.943854</td>\n",
       "      <td>0.403936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.142880</td>\n",
       "      <td>0.945496</td>\n",
       "      <td>0.405674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.143465</td>\n",
       "      <td>0.944992</td>\n",
       "      <td>0.393401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.144786</td>\n",
       "      <td>0.943545</td>\n",
       "      <td>0.439276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.145456</td>\n",
       "      <td>0.944862</td>\n",
       "      <td>0.426906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.144274</td>\n",
       "      <td>0.945707</td>\n",
       "      <td>0.416972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.142340</td>\n",
       "      <td>0.945740</td>\n",
       "      <td>0.436032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.142295</td>\n",
       "      <td>0.945545</td>\n",
       "      <td>0.413382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.944179</td>\n",
       "      <td>0.416454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.151193</td>\n",
       "      <td>0.943236</td>\n",
       "      <td>0.396960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.117800</td>\n",
       "      <td>0.147549</td>\n",
       "      <td>0.944829</td>\n",
       "      <td>0.446583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.150060</td>\n",
       "      <td>0.944016</td>\n",
       "      <td>0.423186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.213779</td>\n",
       "      <td>0.937415</td>\n",
       "      <td>0.194601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.935122</td>\n",
       "      <td>0.050904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.147819</td>\n",
       "      <td>0.944325</td>\n",
       "      <td>0.423375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.148809</td>\n",
       "      <td>0.944683</td>\n",
       "      <td>0.430723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.149918</td>\n",
       "      <td>0.943756</td>\n",
       "      <td>0.428925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.148132</td>\n",
       "      <td>0.945008</td>\n",
       "      <td>0.432931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>0.147464</td>\n",
       "      <td>0.944764</td>\n",
       "      <td>0.449165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.146606</td>\n",
       "      <td>0.946748</td>\n",
       "      <td>0.450411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.148782</td>\n",
       "      <td>0.944732</td>\n",
       "      <td>0.457462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.148280</td>\n",
       "      <td>0.945106</td>\n",
       "      <td>0.460531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.150275</td>\n",
       "      <td>0.944260</td>\n",
       "      <td>0.462359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.146778</td>\n",
       "      <td>0.945463</td>\n",
       "      <td>0.468463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.150422</td>\n",
       "      <td>0.945041</td>\n",
       "      <td>0.462126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.148783</td>\n",
       "      <td>0.945073</td>\n",
       "      <td>0.449837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.149495</td>\n",
       "      <td>0.944325</td>\n",
       "      <td>0.457541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.148351</td>\n",
       "      <td>0.945285</td>\n",
       "      <td>0.455237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.148263</td>\n",
       "      <td>0.945707</td>\n",
       "      <td>0.451454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.153095</td>\n",
       "      <td>0.944537</td>\n",
       "      <td>0.447611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.149380</td>\n",
       "      <td>0.944813</td>\n",
       "      <td>0.433389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.148517</td>\n",
       "      <td>0.945089</td>\n",
       "      <td>0.469609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.944390</td>\n",
       "      <td>0.438424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.149649</td>\n",
       "      <td>0.944862</td>\n",
       "      <td>0.455960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.158131</td>\n",
       "      <td>0.944309</td>\n",
       "      <td>0.441364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.236167</td>\n",
       "      <td>0.936569</td>\n",
       "      <td>0.118816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.162236</td>\n",
       "      <td>0.943902</td>\n",
       "      <td>0.428240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.140691</td>\n",
       "      <td>0.946439</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.140960</td>\n",
       "      <td>0.946537</td>\n",
       "      <td>0.490233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.154752</td>\n",
       "      <td>0.944894</td>\n",
       "      <td>0.447235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.148493</td>\n",
       "      <td>0.944130</td>\n",
       "      <td>0.467617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.148253</td>\n",
       "      <td>0.945626</td>\n",
       "      <td>0.464274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.147509</td>\n",
       "      <td>0.945545</td>\n",
       "      <td>0.471350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.218684</td>\n",
       "      <td>0.937480</td>\n",
       "      <td>0.208685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.236154</td>\n",
       "      <td>0.936390</td>\n",
       "      <td>0.121688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.148551</td>\n",
       "      <td>0.945268</td>\n",
       "      <td>0.477491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.147254</td>\n",
       "      <td>0.946163</td>\n",
       "      <td>0.481928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.147626</td>\n",
       "      <td>0.945870</td>\n",
       "      <td>0.473842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.148178</td>\n",
       "      <td>0.946683</td>\n",
       "      <td>0.471044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.150124</td>\n",
       "      <td>0.944276</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.153613</td>\n",
       "      <td>0.943935</td>\n",
       "      <td>0.441891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>0.150781</td>\n",
       "      <td>0.944780</td>\n",
       "      <td>0.455245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.141621</td>\n",
       "      <td>0.945642</td>\n",
       "      <td>0.498575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.139586</td>\n",
       "      <td>0.946585</td>\n",
       "      <td>0.507718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.148206</td>\n",
       "      <td>0.945528</td>\n",
       "      <td>0.462279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.140443</td>\n",
       "      <td>0.945642</td>\n",
       "      <td>0.506277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.146599</td>\n",
       "      <td>0.945593</td>\n",
       "      <td>0.487909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.146847</td>\n",
       "      <td>0.945496</td>\n",
       "      <td>0.483990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.946683</td>\n",
       "      <td>0.487256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.145675</td>\n",
       "      <td>0.946732</td>\n",
       "      <td>0.488445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.146308</td>\n",
       "      <td>0.946081</td>\n",
       "      <td>0.495895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.140426</td>\n",
       "      <td>0.945366</td>\n",
       "      <td>0.508053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.139409</td>\n",
       "      <td>0.946016</td>\n",
       "      <td>0.509456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.138781</td>\n",
       "      <td>0.946325</td>\n",
       "      <td>0.510746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.140687</td>\n",
       "      <td>0.946634</td>\n",
       "      <td>0.499848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.139354</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.505726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.146539</td>\n",
       "      <td>0.946472</td>\n",
       "      <td>0.481901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.148632</td>\n",
       "      <td>0.945805</td>\n",
       "      <td>0.480112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.151797</td>\n",
       "      <td>0.945870</td>\n",
       "      <td>0.479600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.152637</td>\n",
       "      <td>0.946537</td>\n",
       "      <td>0.471383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.145762</td>\n",
       "      <td>0.947089</td>\n",
       "      <td>0.490767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.145404</td>\n",
       "      <td>0.946423</td>\n",
       "      <td>0.485719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.146235</td>\n",
       "      <td>0.945919</td>\n",
       "      <td>0.495449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.148450</td>\n",
       "      <td>0.945756</td>\n",
       "      <td>0.480212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.146715</td>\n",
       "      <td>0.944846</td>\n",
       "      <td>0.488540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.138458</td>\n",
       "      <td>0.947545</td>\n",
       "      <td>0.503845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.138808</td>\n",
       "      <td>0.946081</td>\n",
       "      <td>0.508158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.139112</td>\n",
       "      <td>0.946748</td>\n",
       "      <td>0.508627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.138496</td>\n",
       "      <td>0.947041</td>\n",
       "      <td>0.517124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.138279</td>\n",
       "      <td>0.947220</td>\n",
       "      <td>0.514943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.137913</td>\n",
       "      <td>0.946846</td>\n",
       "      <td>0.518911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.138073</td>\n",
       "      <td>0.947089</td>\n",
       "      <td>0.524408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.138410</td>\n",
       "      <td>0.946829</td>\n",
       "      <td>0.522906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.138112</td>\n",
       "      <td>0.946683</td>\n",
       "      <td>0.521663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.138566</td>\n",
       "      <td>0.947285</td>\n",
       "      <td>0.515251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.138111</td>\n",
       "      <td>0.946943</td>\n",
       "      <td>0.523858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.138265</td>\n",
       "      <td>0.947073</td>\n",
       "      <td>0.518562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.138343</td>\n",
       "      <td>0.946780</td>\n",
       "      <td>0.513164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.138482</td>\n",
       "      <td>0.946650</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.138056</td>\n",
       "      <td>0.947057</td>\n",
       "      <td>0.519906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.145133</td>\n",
       "      <td>0.946114</td>\n",
       "      <td>0.500753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.145890</td>\n",
       "      <td>0.945496</td>\n",
       "      <td>0.495029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.112200</td>\n",
       "      <td>0.141706</td>\n",
       "      <td>0.946520</td>\n",
       "      <td>0.509909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.138325</td>\n",
       "      <td>0.947057</td>\n",
       "      <td>0.519197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.144747</td>\n",
       "      <td>0.946423</td>\n",
       "      <td>0.499772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.145825</td>\n",
       "      <td>0.945724</td>\n",
       "      <td>0.493321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.138197</td>\n",
       "      <td>0.946911</td>\n",
       "      <td>0.516511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.147656</td>\n",
       "      <td>0.945431</td>\n",
       "      <td>0.488103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7625, training_loss=0.12811356942453345, metrics={'train_runtime': 61884.934, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.123, 'total_flos': 8082442648473600.0, 'train_loss': 0.12811356942453345, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13,
   "source": [
    "# Train the model for 5 epochs; evaluate every 50 steps\n",
    "trainer.train()"
   ],
   "id": "22e02596f9362bb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def classify(s):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input string\n",
    "    tok_title = tokenizer(\n",
    "        s,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Move tokenized input to the device\n",
    "    tok_title = {k: v.to(device) for k, v in tok_title.items()}\n",
    "\n",
    "    # Run model on tokenized input\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tok_title)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.sigmoid(logits)\n",
    "\n",
    "        # Output the 5 most probable arXiv categories with corresp probabilities\n",
    "        top5_prob, top5_idx = torch.topk(preds, 5, dim=1)\n",
    "\n",
    "    for x in range(5):\n",
    "        print(f\"{id2label[top5_idx[0, x].item()]}: {top5_prob[0, x].item() * 100:.2f}%\")"
   ],
   "id": "1d5ad11bc5f87826"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T14:11:48.610157Z",
     "start_time": "2025-09-03T14:11:47.243770Z"
    }
   },
   "cell_type": "code",
   "source": "classify(\"Super Yang-Mills Theory on Toric Orbifolds\")",
   "id": "1321c2af50297a5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hep-th: 71.24%\n",
      "math.MP: 34.33%\n",
      "math-ph: 32.58%\n",
      "hep-ph: 8.50%\n",
      "cond-mat.str-el: 4.82%\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T13:18:55.376365Z",
     "start_time": "2025-09-03T13:18:54.448954Z"
    }
   },
   "cell_type": "code",
   "source": "classify(\"Exploring new fractional quantum Hall states\")",
   "id": "121c4b307a5b067a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond-mat.mes-hall: 51.69%\n",
      "cond-mat.str-el: 40.40%\n",
      "quant-ph: 25.42%\n",
      "cond-mat.mtrl-sci: 5.83%\n",
      "cond-mat.stat-mech: 3.47%\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T14:11:25.690229Z",
     "start_time": "2025-09-03T14:11:25.640497Z"
    }
   },
   "cell_type": "code",
   "source": "classify(\"Data science in the era of reinforcement learning\")",
   "id": "5d859f5805d1b57f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat.ML: 65.71%\n",
      "cs.LG: 44.37%\n",
      "cs.AI: 19.67%\n",
      "quant-ph: 2.53%\n",
      "cs.CV: 1.85%\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1b47c6f28b6471f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
